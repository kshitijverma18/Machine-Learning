{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT BY PRAKNAM S^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Random shuffle the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(values,ratio):\n",
    "    random.shuffle(values)\n",
    "    limit=int(math.ceil(len(values) * ratio))\n",
    "    train = values[: limit]\n",
    "    test = values[limit: ]\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    for i in train:\n",
    "        x_train.append(i[: 6])\n",
    "        y_train.append(i[6])\n",
    "    for i in test:\n",
    "        x_test.append(i[: 6])\n",
    "        y_test.append(i[6])\n",
    "        x_train = np.array([np.array(i) for i in x_train])\n",
    "        \n",
    "    ones=np.ones((x_train.shape[0],1))\n",
    "    y_train = np.array([np.array(i) for i in y_train])\n",
    "    x_test = np.array([np.array(i) for i in x_test])\n",
    "    ones=np.ones((x_test.shape[0],1))\n",
    "    y_test = np.array([np.array(i) for i in y_test])    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Standardize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(lines):\n",
    "    for i in range(6):\n",
    "        tmp = []\n",
    "        for line in lines:\n",
    "            tmp.append(line[i])\n",
    "        tmp = np.array(tmp)\n",
    "        tmp_mean = tmp.mean()\n",
    "        tmp_std = tmp.std(ddof = 0)\n",
    "        for line in lines:            \n",
    "            line[i] = (line[i] - tmp_mean) / tmp_std\n",
    "            \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) *(1-sigmoid (x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      att1  att2  att3        att4      att5         att6  class\n",
      "0      1.0   0.0   0.0  133.150861  1.311693  1620.221779      1\n",
      "1      1.0   0.0   0.0  126.724861  1.302745  1609.334822      1\n",
      "2      1.0   0.0   0.0  131.173861  1.319031  1568.978435      1\n",
      "3      1.0   0.0   0.0  129.478861  1.270878  1695.055281      1\n",
      "4      1.0   0.0   0.0  127.262861  1.329637  1647.720235      1\n",
      "...    ...   ...   ...         ...       ...          ...    ...\n",
      "1995   1.0   1.0   1.0  157.498861  1.655794  5326.025889     10\n",
      "1996   1.0   1.0   1.0  152.404861  1.620345  5243.267754     10\n",
      "1997   1.0   1.0   1.0  134.672861  1.541987  3766.763222     10\n",
      "1998   1.0   1.0   1.0  142.926861  1.426381  4118.327320     10\n",
      "1999   1.0   1.0   1.0  133.920861  1.564621  3808.021317     10\n",
      "\n",
      "[2000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset_NN.csv',header=0,delimiter=\",\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "att1 = list(data['att1'])\n",
    "att2 = list(data['att2'])\n",
    "att3 = list(data['att3'])\n",
    "att4 = list(data['att4'])\n",
    "att5 = list(data['att5'])\n",
    "att6 = list(data['att6'])\n",
    "actual = list(data['class'])\n",
    "values = []\n",
    "\n",
    "for i in range(len(actual)):\n",
    "    values.append([att1[i], att2[i], att3[i], att4[i],att5[i],att6[i]])\n",
    "    \n",
    "values = feature_scaling(values)\n",
    "\n",
    "for i in range(len(actual)):\n",
    "    values[i].append(actual[i])\n",
    "\n",
    "x_train, y_train, x_test, y_test=random_shuffle(values,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x,wh,wo,bh,bo):\n",
    "    \n",
    "    # Phase 1\n",
    "    zh = np.dot(x, weight) + bias\n",
    "    ah = sigmoid(zh)\n",
    "\n",
    "    # Phase 2\n",
    "    zo = np.dot(ah, wo) + bo\n",
    "    ao = softmax(zo)\n",
    "    \n",
    "    # print(ao)\n",
    "    ao_list = ao.tolist()\n",
    "    predicted = []\n",
    "    for i in range(len(y_train)):\n",
    "        max_pos = ao_list[i].index(max(ao_list[i]))\n",
    "        predicted.append(max_pos+1)\n",
    "    count = 0;\n",
    "    for i in range(len(y_train)):\n",
    "        if(predicted[i]==y_train[i]):\n",
    "            count=count+1\n",
    "    print(\"Training Accuracy=\",count/len(y_train)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function value:  3274.0613385596826\n",
      "Loss function value:  1218.9982947996373\n",
      "Loss function value:  1036.5267366585017\n",
      "Loss function value:  965.3133108774032\n",
      "Loss function value:  918.5819451732386\n",
      "Loss function value:  888.1345749639702\n",
      "Loss function value:  870.6860672126567\n",
      "Loss function value:  859.9550045491432\n",
      "Loss function value:  852.4236007485663\n",
      "Loss function value:  846.6120463783293\n",
      "Loss function value:  841.8598414910366\n",
      "Loss function value:  837.8247158104796\n",
      "Loss function value:  834.3065901648188\n",
      "Loss function value:  831.19116095765\n",
      "Loss function value:  828.4221826242737\n",
      "Loss function value:  825.963506046228\n",
      "Loss function value:  823.7756054113145\n",
      "Loss function value:  821.8171359785474\n",
      "Loss function value:  820.0508717464504\n",
      "Loss function value:  818.4458224815664\n",
      "Loss function value:  816.9769448412469\n",
      "Loss function value:  815.6241387916284\n",
      "Loss function value:  814.3712163319492\n",
      "Loss function value:  813.2050344293314\n",
      "Loss function value:  812.1148143566887\n",
      "Loss function value:  811.0916215599798\n",
      "Loss function value:  810.1279718490221\n",
      "Loss function value:  809.2175332912275\n",
      "Loss function value:  808.354899445874\n",
      "Loss function value:  807.5354155164323\n",
      "Loss function value:  806.755043816285\n",
      "Loss function value:  806.0102585988864\n",
      "Loss function value:  805.2979629897025\n",
      "Loss function value:  804.6154227018949\n",
      "Loss function value:  803.9602126181004\n",
      "Loss function value:  812.312166690006\n",
      "Loss function value:  807.0873441748171\n",
      "Loss function value:  806.4629287416287\n",
      "Loss function value:  805.9717045367664\n",
      "Loss function value:  805.5075186478343\n",
      "Loss function value:  805.0572709487382\n",
      "Loss function value:  804.6163899111061\n",
      "Loss function value:  804.1827889369853\n",
      "Loss function value:  803.755488184582\n",
      "Loss function value:  803.3340917157346\n",
      "Loss function value:  802.9185369390767\n",
      "Loss function value:  802.5089579685389\n",
      "Loss function value:  802.1056063247045\n",
      "Loss function value:  801.7088039217593\n",
      "Loss function value:  801.31891502338\n",
      "Loss function value:  800.9363291166542\n",
      "Loss function value:  800.5614495471048\n",
      "Loss function value:  800.1946846700419\n",
      "Loss function value:  799.8364396685218\n",
      "Loss function value:  799.4871082009629\n",
      "Loss function value:  799.1470637156269\n",
      "Loss function value:  798.8166506513419\n",
      "Loss function value:  798.4961759044759\n",
      "Loss function value:  798.1859009618886\n",
      "Loss function value:  797.8860350493577\n",
      "Loss function value:  797.5967295732506\n",
      "Loss function value:  797.3180740641941\n",
      "Loss function value:  797.0500937703546\n",
      "Loss function value:  796.7927489894\n",
      "Loss function value:  796.5459361645346\n",
      "Loss function value:  796.309490696543\n",
      "Loss function value:  796.0831913418453\n",
      "Loss function value:  795.8667659832065\n",
      "Loss function value:  795.6598984856654\n",
      "Loss function value:  795.462236296416\n",
      "Loss function value:  795.2733984219632\n",
      "Loss function value:  795.0929834220418\n",
      "Loss function value:  794.9205770951485\n",
      "Loss function value:  794.7557595882733\n",
      "Loss function value:  794.5981117341714\n",
      "Loss function value:  794.4472204935967\n",
      "Loss function value:  794.3026834492193\n",
      "Loss function value:  794.1641123567463\n",
      "Loss function value:  794.0311358041802\n",
      "Loss function value:  793.9034010615904\n",
      "Loss function value:  793.7805752224051\n",
      "Loss function value:  793.6623457451296\n",
      "Loss function value:  793.5484205041337\n",
      "Loss function value:  793.4385274520636\n",
      "Loss function value:  793.332413986748\n",
      "Loss function value:  793.2298461038184\n",
      "Loss function value:  793.1306074040107\n",
      "Loss function value:  793.034498012061\n",
      "Loss function value:  792.9413334529432\n",
      "Loss function value:  792.8509435211632\n",
      "Loss function value:  792.7631711701183\n",
      "Loss function value:  792.6778714411695\n",
      "Loss function value:  792.594910445994\n",
      "Loss function value:  792.5141644108666\n",
      "Loss function value:  792.4355187876573\n",
      "Loss function value:  792.3588674333495\n",
      "Loss function value:  792.2841118576564\n",
      "Loss function value:  792.2111605367134\n",
      "Loss function value:  792.1399282897112\n",
      "Loss function value:  792.0703357146217\n",
      "Loss function value:  792.0023086787879\n",
      "Loss function value:  791.9357778599336\n",
      "Loss function value:  791.8706783332088\n",
      "Loss function value:  791.806949199947\n",
      "Loss function value:  791.7445332540656\n",
      "Loss function value:  791.6833766822641\n",
      "Loss function value:  791.6234287944883\n",
      "Loss function value:  791.5646417813988\n",
      "Loss function value:  791.5069704959153\n",
      "Loss function value:  791.450372256171\n",
      "Loss function value:  791.3948066674905\n",
      "Loss function value:  791.340235461294\n",
      "Loss function value:  791.286622349003\n",
      "Loss function value:  791.2339328893113\n",
      "Loss function value:  791.182134367329\n",
      "Loss function value:  791.1311956842956\n",
      "Loss function value:  791.0810872567281\n",
      "Loss function value:  791.0317809239796\n",
      "Loss function value:  790.9832498633442\n",
      "Loss function value:  790.9354685119063\n",
      "Loss function value:  790.8884124944602\n",
      "Loss function value:  790.8420585568992\n",
      "Loss function value:  790.7963845045349\n",
      "Loss function value:  790.7513691448817\n",
      "Loss function value:  790.7069922344908\n",
      "Loss function value:  790.6632344294711\n",
      "Loss function value:  790.6200772393602\n",
      "Loss function value:  790.5775029840689\n",
      "Loss function value:  790.5354947536355\n",
      "Loss function value:  790.4940363705477\n",
      "Loss function value:  790.4531123544568\n",
      "Loss function value:  790.4127078890531\n",
      "Loss function value:  790.3728087909776\n",
      "Loss function value:  790.3334014805939\n",
      "Loss function value:  790.2944729544884\n",
      "Loss function value:  790.2560107595776\n",
      "Loss function value:  790.218002968705\n",
      "Loss function value:  790.1804381576296\n",
      "Loss function value:  790.1433053833011\n",
      "Loss function value:  790.1065941633489\n",
      "Loss function value:  790.070294456689\n",
      "Loss function value:  790.0343966451867\n",
      "Loss function value:  789.9988915163061\n",
      "Loss function value:  789.9637702466747\n",
      "Loss function value:  789.9290243865139\n",
      "Loss function value:  789.8946458448767\n",
      "Loss function value:  789.8606268756462\n",
      "Loss function value:  789.8269600642354\n",
      "Loss function value:  789.7936383149702\n",
      "Loss function value:  789.7606548390806\n",
      "Loss function value:  789.7280031432888\n",
      "Loss function value:  789.6956770189527\n",
      "Loss function value:  789.6636705317135\n",
      "Loss function value:  789.6319780116474\n",
      "Loss function value:  789.6005940438552\n",
      "Loss function value:  789.5695134594948\n",
      "Loss function value:  789.5387313272103\n",
      "Loss function value:  789.5082429449288\n",
      "Loss function value:  789.4780438320292\n",
      "Loss function value:  789.4481297218256\n",
      "Loss function value:  789.4184965543633\n",
      "Loss function value:  789.3891404695122\n",
      "Loss function value:  789.3600578003177\n",
      "Loss function value:  789.3312450666141\n",
      "Loss function value:  789.3026989688694\n",
      "Loss function value:  789.2744163822417\n",
      "Loss function value:  789.2463943508482\n",
      "Loss function value:  789.218630082217\n",
      "Loss function value:  789.1911209419065\n",
      "Loss function value:  789.1638644482931\n",
      "Loss function value:  789.1368582675022\n",
      "Loss function value:  789.1101002084662\n",
      "Loss function value:  789.0835882181116\n",
      "Loss function value:  789.0573203766526\n",
      "Loss function value:  789.0312948929898\n",
      "Loss function value:  789.0055101001848\n",
      "Loss function value:  788.9799644510257\n",
      "Loss function value:  788.9546565136535\n",
      "Loss function value:  788.929584967258\n",
      "Loss function value:  788.9047485978106\n",
      "Loss function value:  788.8801462938525\n",
      "Loss function value:  788.8557770423133\n",
      "Loss function value:  788.8316399243581\n",
      "Loss function value:  788.8077341112547\n",
      "Loss function value:  788.7840588602576\n",
      "Loss function value:  788.7606135104975\n",
      "Loss function value:  788.7373974788763\n",
      "Loss function value:  788.7144102559627\n",
      "Loss function value:  788.6916514018765\n",
      "Loss function value:  788.6691205421657\n",
      "Loss function value:  788.6468173636671\n",
      "Loss function value:  788.6247416103579\n",
      "Loss function value:  788.6028930791749\n",
      "Loss function value:  788.5812716158266\n",
      "Loss function value:  788.5598771105759\n",
      "Loss function value:  788.5387094939979\n",
      "Loss function value:  788.5177687327204\n",
      "Loss function value:  788.4970548251371\n",
      "Loss function value:  788.4765677971033\n",
      "Loss function value:  788.4563076976071\n",
      "Loss function value:  788.4362745944273\n",
      "Loss function value:  788.4164685697738\n",
      "Loss function value:  788.3968897159191\n",
      "Loss function value:  788.3775381308218\n",
      "Loss function value:  788.3584139137474\n",
      "Loss function value:  788.3395171608948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function value:  788.3208479610277\n",
      "Loss function value:  788.3024063911316\n",
      "Loss function value:  788.2841925120795\n",
      "Loss function value:  788.266206364339\n",
      "Loss function value:  788.248447963719\n",
      "Loss function value:  788.230917297154\n",
      "Loss function value:  788.2136143185572\n",
      "Loss function value:  788.1965389447278\n",
      "Loss function value:  788.179691051346\n",
      "Loss function value:  788.1630704690475\n",
      "Loss function value:  788.1466769795894\n",
      "Loss function value:  788.1305103121333\n",
      "Loss function value:  788.1145701396412\n",
      "Loss function value:  788.0988560753985\n",
      "Loss function value:  788.0833676696768\n",
      "Loss function value:  788.0681044065446\n",
      "Loss function value:  788.0530657008476\n",
      "Loss function value:  788.0382508953404\n",
      "Loss function value:  788.0236592580193\n",
      "Loss function value:  788.0092899796309\n",
      "Loss function value:  787.995142171387\n",
      "Loss function value:  787.9812148628815\n",
      "Loss function value:  787.9675070002315\n",
      "Loss function value:  787.9540174444344\n",
      "Loss function value:  787.9407449699554\n",
      "Loss function value:  787.9276882635572\n",
      "Loss function value:  787.9148459233684\n",
      "Loss function value:  787.9022164581924\n",
      "Loss function value:  787.889798287071\n",
      "Loss function value:  787.877589739102\n",
      "Loss function value:  787.8655890534983\n",
      "Loss function value:  787.8537943799142\n",
      "Loss function value:  787.8422037790172\n",
      "Loss function value:  787.8308152233069\n",
      "Loss function value:  787.8196265981935\n",
      "Loss function value:  787.8086357033126\n",
      "Loss function value:  787.7978402540764\n",
      "Loss function value:  787.7872378834745\n",
      "Loss function value:  787.7768261440822\n",
      "Loss function value:  787.7666025103049\n",
      "Loss function value:  787.7565643808257\n",
      "Loss function value:  787.7467090812497\n",
      "Loss function value:  787.7370338669555\n",
      "Loss function value:  787.7275359261048\n",
      "Training Accuracy= 76.78571428571429\n",
      "Testing Accuracy= 74.16666666666667\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 6\n",
    "output_nodes = 10\n",
    "\n",
    "hidden_nodes = 4\n",
    "output_labels = 10\n",
    "\n",
    "\n",
    "wh = np.random.rand(input_nodes,hidden_nodes)\n",
    "bh = np.random.randn(hidden_nodes)\n",
    "\n",
    "wo = np.random.rand(hidden_nodes,output_labels)\n",
    "bo = np.random.randn(output_labels)\n",
    "error_cost = []\n",
    "one_hot_labels = np.zeros((len(y_train), output_labels))\n",
    "for i in range(len(y_train)):\n",
    "    index = y_train[i];\n",
    "    one_hot_labels[i][index-1]=1\n",
    "    \n",
    "for epoch in range(50000):\n",
    "    \n",
    "    ############# feedforward\n",
    "\n",
    "    # Phase 1\n",
    "    zh = np.dot(x_train, wh) + bh\n",
    "    ah = sigmoid(zh)\n",
    "\n",
    "    # Phase 2\n",
    "    zo = np.dot(ah, wo) + bo\n",
    "    ao = softmax(zo)\n",
    "\n",
    "\n",
    "    \n",
    "    ########## Back Propagation\n",
    "\n",
    "########## Phase 1\n",
    "    dcost_dzo = ao - one_hot_labels # d_cost/d_ao * d_ao/d_zo\n",
    "    dzo_dwo = ah \n",
    "    dcost_wo = np.dot(dzo_dwo.T, dcost_dzo)\n",
    "    dcost_bo = dcost_dzo\n",
    "\n",
    "########## Phases 2\n",
    "    dzo_dah = wo\n",
    "    dcost_dah = np.dot(dcost_dzo , dzo_dah.T)\n",
    "    dah_dzh = sigmoid_der(zh)\n",
    "    dzh_dwh = x_train\n",
    "    dcost_wh = np.dot(dzh_dwh.T, dah_dzh * dcost_dah)\n",
    "    dcost_bh = dcost_dah * dah_dzh\n",
    "\n",
    "    # Update Weights ================\n",
    "\n",
    "    wh -= lr * dcost_wh\n",
    "    bh -= lr * dcost_bh.sum(axis=0)\n",
    "\n",
    "    wo -= lr * dcost_wo\n",
    "    bo -= lr * dcost_bo.sum(axis=0)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        loss = np.sum(-one_hot_labels * np.log(ao))\n",
    "        print('Loss function value: ', loss)\n",
    "        error_cost.append(loss)\n",
    "\n",
    "# Phase 1\n",
    "zh = np.dot(x_train, wh) + bh\n",
    "ah = sigmoid(zh)\n",
    "\n",
    "# Phase 2\n",
    "zo = np.dot(ah, wo) + bo\n",
    "ao = softmax(zo)\n",
    "# print(ao)\n",
    "ao_list = ao.tolist()\n",
    "predicted = []\n",
    "for i in range(len(y_train)):\n",
    "    max_pos = ao_list[i].index(max(ao_list[i]))\n",
    "    predicted.append(max_pos+1)\n",
    "count = 0;\n",
    "for i in range(len(y_train)):\n",
    "    if(predicted[i]==y_train[i]):\n",
    "        count=count+1\n",
    "print(\"Training Accuracy=\",count/len(y_train)*100)\n",
    "\n",
    "\n",
    "# TESTING ACCURACY\n",
    "testing_a = []\n",
    "# Phase 1\n",
    "zh = np.dot(x_test, wh) + bh\n",
    "ah = sigmoid(zh)\n",
    "\n",
    "# Phase 2\n",
    "zo = np.dot(ah, wo) + bo\n",
    "ao = softmax(zo)\n",
    "# print(ao)\n",
    "ao_list = ao.tolist()     \n",
    "for i in range(len(y_test)):\n",
    "    max_pos = ao_list[i].index(max(ao_list[i]))\n",
    "    testing_a.append(max_pos+1)\n",
    "\n",
    "\n",
    "count = 0;    \n",
    "# print(len(testing_a))\n",
    "# print(testing_a)\n",
    "for i in range(len(y_test)):\n",
    "    if(testing_a[i]==y_test[i]):\n",
    "        count=count+1\n",
    "print(\"Testing Accuracy=\",count/len(y_test)*100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
